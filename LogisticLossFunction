1. Why square error is not properly for logistic regression?
  Using square error in logistic regression is not suitable mainly because logistic regression deals with categorical outcomes (like yes/no, 0/1), not continuous values. Here's why square error isn't a good fit in plain language:
    Mismatched Outcomes: Logistic regression predicts probabilities, which are between 0 and 1. Square error, however, is more suited for continuous outcomes that can vary widely. It's like using a ruler to measure the weight of something - it's just not the right tool for the job.
    Non-Linear Nature: The relationship in logistic regression is not straight-line (linear), but S-shaped (sigmoid). Using square error assumes a linear relationship, which can lead to poor performance in predictions because it doesn't match the actual shape of the data.
    Sensitive to Outliers: Square error is heavily influenced by outliers (extreme values). In logistic regression, this can lead to misleading results because it might overemphasize rare events.
    Probability Calibration: Logistic regression aims to provide calibrated probabilities. Square error does not focus on probabilities directly, so it may not provide accurate probability estimates.
    Convexity Issues: For technical reasons, using square error can lead to optimization problems that are hard to solve. Logistic regression with a logistic (or log) loss function ensures a smooth and convex optimization landscape, making it easier to find the best solution.
  In summary, square error is like using a hammer on a screw - it's not the right tool for the job when working with logistic regression, which deals with probabilities and categorical outcomes. The logistic loss function is a much better fit for these tasks.

2. What is logistic loss function?
  The logistic loss function, often used in logistic regression, is a way to measure how well a model's predictions match the actual outcomes, specifically when those outcomes are categories like "yes" or "no". Let's break it down in plain language:
    Binary Outcomes: Logistic regression is used when your data can be split into two categories, like passing/failing, sick/healthy, etc. The logistic loss function is specially designed for these kinds of binary outcomes.
    Predicting Probabilities: Instead of just predicting "yes" or "no", logistic regression predicts the probability of "yes" (or "no"). For example, rather than just saying a patient has a disease, it might say there's a 70% chance.
    The "Loss" Part: "Loss" refers to how far off a prediction is from the actual outcome. Imagine you're throwing darts. Each time you miss the bullseye, that's a "loss". In logistic regression, the "loss" is how far off the model's probability estimate is from the actual outcome.
    Encouraging Accurate Probabilities: The logistic loss function specifically penalizes predictions that are confident but wrong. So, if the model says there's a 90% chance of something happening and it doesn't happen, that's a big penalty. This encourages the model not just to be right, but to be reasonably confident in its predictions.
    Math Behind the Scenes: Mathematically, the logistic loss function has a special formula that crunches the numbers to figure out the loss. It's designed so that the calculations work well with probabilities and binary outcomes.
  In summary, the logistic loss function is a way to keep score of how well a logistic regression model is doing at predicting binary outcomes, with a focus on getting the probabilities right, especially when it comes to being confident in its predictions.
