1. What is neuron network?
  A neural network, in plain language, is a computer system that's designed to work kind of like the human brain. Imagine your brain as a huge network of cells called neurons, each connected to many others. These neurons communicate with each other by sending signals. When you learn something new, these connections get stronger, helping you remember and understand things better.
  Now, in a computer, a neural network tries to mimic this process. It's made up of artificial neurons - tiny, simple programs not much different than the cells in your brain. These artificial neurons are connected to each other and they send signals back and forth. 
  When a neural network is learning, it's actually adjusting the strength of these connections based on the information it's given. For example, if you show it lots of pictures of cats and tell it, "These are all cats," it starts to recognize patterns that define what a cat looks like. The next time it sees a cat, even if it's a bit different from the ones it learned from, it can still say, "Hey, that's probably a cat!"
  Neural networks are used in many things like recommending videos on streaming services, recognizing faces in photos, helping cars drive themselves, and even in medicine to help diagnose diseases. They're powerful tools because they can learn and adapt, much like our brains do.

2. How does neuron network work?
  Input: It all starts with data. Let's say you want the neural network to recognize whether a photo has a cat in it. You start by showing it lots of photos, and each photo is an 'input'.
Layers of Artificial Neurons: Think of these layers as a series of filters. In the first layer, the network might learn to recognize simple shapes like edges and corners. The next layer might put these shapes together to recognize parts of a cat, like ears or eyes. And the final layer might combine all these parts to recognize a whole cat.
Connections with 'Weights': Each artificial neuron in these layers is connected to many others. These connections have 'weights', which are like knobs that can be turned up or down. In the beginning, these weights are set randomly.
  Learning Process: When you show the network a photo, it makes a guess about whether there's a cat in it. At first, it's probably wrong because the weights are all random. But then, you tell it, "This one had a cat" or "This one didn't." The network then adjusts the weights of its connections to be a little more accurate next time.
  Backpropagation: This is a fancy term for how the network learns from its mistakes. If the network guessed wrong, it goes back and adjusts the weights. It's like the network is thinking, "If I see this kind of pattern, it's more likely to be a cat, so I'll pay more attention to it next time."
Iteration and Improvement: This process of guessing, getting feedback, and adjusting repeats many times. Each time, the network gets a bit better at recognizing cats.
  Output: After lots of learning, when you show the network a new photo, it can accurately tell you whether there's a cat in it or not.
  In summary, a neural network learns by looking at lots of examples, making guesses, and continuously improving itself based on whether its guesses are right or wrong. It's like how a child learns to recognize animals in a picture book - through repeated exposure and feedback.

3. Neuron Network develop history.
  The development history of neural networks is a fascinating journey through time, blending concepts from neuroscience, mathematics, engineering, and computer science. Here's a simplified timeline:
  1940s - Early Concepts: The idea of neural networks began with the goal of creating machines that could mimic the human brain. In 1943, Warren McCulloch, a neurophysiologist, and Walter Pitts, a mathematician, published a paper describing how neurons in the brain might work and proposed a simple neural network model using electrical circuits. This was the theoretical foundation.
  1950s - Perceptrons: The 1950s saw the creation of the first conceptual models of neural networks. Frank Rosenblatt, a psychologist, invented the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network. The perceptron was able to learn through supervised training.
  1960s - XOR Problem and AI Winter: A major setback occurred in the 1960s when Marvin Minsky and Seymour Papert published a book demonstrating that perceptrons couldn't solve certain types of problems, like the XOR problem (a simple problem of classification). This led to the first "AI Winter," a period where funding and interest in neural network research dropped significantly.
  1980s - Renewed Interest and Backpropagation: Interest in neural networks was revived in the 1980s, thanks in part to the development of the backpropagation algorithm, which efficiently trained multi-layer networks. This algorithm was key in solving the XOR problem and opened up new possibilities for neural network applications.
  1990s - Rise of Practical Applications: The 1990s saw the rise of practical applications for neural networks in various fields like finance, medicine, and engineering. This was due to improved training techniques, better understanding of theoretical aspects, and increasing computational power.
  2000s - Deep Learning and Big Data: The 2000s witnessed the emergence of deep learning, a subset of neural networks with many layers (hence "deep") that can learn from a vast amount of data ("big data"). This era saw significant improvements in processing power and data availability, which were crucial for training deep neural networks.
  2010s and Beyond - Mainstream Adoption: The last decade has seen neural networks, especially deep learning, become mainstream in technology. They're used in everything from image and speech recognition to self-driving cars and playing complex games like Go and chess at a superhuman level.
  The history of neural networks is marked by periods of hype, disappointment, and resurgence. Today, they are at the forefront of AI research and application, driving many of the technologies we use daily.
